---
title: "6-decision-tree"
output: html_document
---

```{r}
# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())

# Set global knitr chunk options
knitr::opts_chunk$set(
  warning = FALSE, 
  message = FALSE
)
```

```{r}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(caret)
library(naniar)
library(pdp)
library(rsample)
library(recipes)
library(vip)
```

### Partitioning

-   CART uses binary recursive partitioning (itâ€™s recursive because each split or rule depends on the the splits above it). The objective at each node is to ï¬nd the â€œbestâ€ feature (xiï¿½ ) to partition the remaining data into one of two regions (ğ‘…1 and ğ‘…2 ) such that the overall error between the actual response (ğ‘¦uï¿½ ) and the predicted constant (ğ‘uï¿½ ) is minimized. For regression problems, the objective function to minimize is the total SSE. For classiï¬cation problems, the partitioning is usually made to maximize the reduction in cross-entropy or the Gini index.

```{r}
ames <- AmesHousing::make_ames()

ames$Date <- as.Date(paste(ames$Year_Sold, ames$Mo_Sold, "01", sep = "-"))

set.seed(40)  # for reproducibility
split  <- rsample::initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- rsample::training(split)
ames_test   <- rsample::testing(split)
```

```{r}
ames_dt1 <- rpart(
  formula = Sale_Price ~ .,
  data = ames_train,
  method = "anova"
)

ames_dt1
```

```{r}
rpart.plot(ames_dt1)
```

```{r}
plotcp(ames_dt1)
```

-   Behind the scenes rpart() is automatically applying a range of cost complexity (alpha values to prune the tree). To compare the error of each alpha value, rpart() performs a 10-fold CV (default). In this example, the diminishing returns after 10 terminal nodes.

-   ğ‘¦-axis is the CV error, lower ğ‘¥-axis is the cost complexity (ğ›¼) value, upper ğ‘¥-axis is the number of terminal nodes (i.e., tree size = \|ğ‘‡\|). You may also notice the dashed line which goes through the point \|ğ‘‡\| = 8. Itâ€™s common to instead use the smallest tree within 1 standard error (SE) of the minimum CV error (this is called the 1-SE rule). Thus, we could use a tree with 8 terminal nodes and reasonably expect to experience similar results within a small margin of error.

-   To illustrate the point of selecting a tree with 11 terminal nodes (or 8 if you go by the 1-SE rule), we can force rpart() to generate a full tree by setting cp = 0 (no penalty results in a fully grown tree). Figure below shows that after 11 terminal nodes, we see diminishing returns in error reduction as the tree grows deeper. Thus, we can signiï¬cantly prune our tree and still achieve minimal expected error.

```{r}
ames_dt2 <- rpart(
  formula = Sale_Price ~ .,
  data = ames_train,
  method = "anova",
  control = list(cp = 0, xval = 10)
)

plotcp(ames_dt2)
abline(v = 11, lty = "dashed")
```

```{r}
ames_dt2$cptable %>% head()
```

```{r}
ames_dt3 <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "rpart",
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 20
)

ames_dt3
```

```{r}
ggplot(ames_dt3) %>% plotly::ggplotly()
```

```{r}
vip(ames_dt3, num_features = 32, geom = "point", aesthetics = list(color = "red")) %>% plotly::ggplotly()
```

```{r}
# Construct partial dependence plots
p1 <- partial(ames_dt3, pred.var = "Gr_Liv_Area") %>% autoplot()
p2 <- partial(ames_dt3, pred.var = "Year_Built") %>% autoplot()
p3 <- partial(ames_dt3, pred.var = c("Gr_Liv_Area", "Year_Built")) %>%
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE,
              colorkey = TRUE, screen = list(z = -20, x = -60))

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```
