---
title: "regularized-regression"
output: html_document
---

```{r}
library(tidyverse)
library(caret)
library(h2o)
library(glmnet)
library(naniar)
library(vip)
library(TSstudio)
library(plotly)
library(ROCR)
library(recipes)
library(rsample)
library(timetk)
pacman::p_load(epirhandbook)
```

```{r}
ames <- AmesHousing::make_ames()

ames$Date <- as.Date(paste(ames$Year_Sold, ames$Mo_Sold, "01", sep = "-"))

set.seed(40)  # for reproducibility
split  <- rsample::initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- rsample::training(split)
ames_test   <- rsample::testing(split)
```

```{r}
names(ames)
```

```{r}
ames_sub <- ames_train %>%
  filter(Gr_Liv_Area > 1000 & Gr_Liv_Area < 3000) %>%
  sample_frac(.5)

model1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_sub)
```

```{r}
model1 %>%
  broom::augment() %>%
  ggplot(aes(Gr_Liv_Area, Sale_Price)) +
  geom_segment(aes(x = Gr_Liv_Area, y = Sale_Price, 
                   xend = Gr_Liv_Area, yend = .fitted),
               alpha = .3) +
  geom_point(size = 1, color = "red") +
  geom_smooth(se = FALSE, method = "lm") +
  scale_y_continuous(labels = scales::dollar)
```

```{r}
# Augment model data
augmented_data <- model1 %>% broom::augment()

# Create interactive plot with plotly
plot_ly(augmented_data) %>%
  add_segments(
    x = ~Gr_Liv_Area, y = ~Sale_Price,
    xend = ~Gr_Liv_Area, yend = ~.fitted,
    opacity = 0.3, line = list(color = "gray")
  ) %>%
  add_markers(
    x = ~Gr_Liv_Area, y = ~Sale_Price,
    marker = list(color = "red", size = 5)
  ) %>%
  add_lines(
    x = ~Gr_Liv_Area, y = ~.fitted,
    line = list(color = "blue", width = 2),
    name = "Fitted Line"
  ) %>%
  layout(
    title = "Living Area vs Sale Price",
    xaxis = list(title = "Above Ground Living Area (sq ft)"),
    yaxis = list(title = "Sale Price", tickformat = "$,.0f")
  )
```

```{r}
# Create a Date column with the first day of each month
# ames_train$Date <- as.Date(paste(ames_train$Year_Sold, ames_train$Mo_Sold, "01", sep = "-"))
# ames_test$Date <- as.Date(paste(ames_test$Year_Sold, ames_test$Mo_Sold, "01", sep = "-"))

# ames_train$Date <- as.Date(as.character(ames_train$Date), format = "%Y%m%d")
# ames_test$Date <- as.Date(as.character(ames_test$Date), format = "%Y%m%d")

ames_time_train <- ames_train %>% select(Date, Sale_Price)
ames_time_test <- ames_test %>% select(Date, Sale_Price)

# Convert to time series object
price_ts <- tk_ts(ames_time_train$Sale_Price, 
                   start = c(lubridate::year(min(ames$Date)), 
                             lubridate::month(min(ames$Date))), 
                   frequency = 12)

ts_plot(price_ts, line.mode = "lines", width = 2,
        Xtitle = "Year",
        Ytitle = "Sale Prices",
        title = "Monthly Sale Prices",
        Ygrid = TRUE,
        Xgrid = TRUE)
```

```{r}
df <- model1 %>% broom::augment()

plot_ly(data = df, x = df$Gr_Liv_Area, y = df$Sale_Price,
        yaxis = "Sale Price", xaxis = "Gr Living Area")
```

### Ridge Regression

-   However, ridge regression does not perform feature selection and will retain all available features in the Ô¨Ånal model. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less inÔ¨Çuential variables may create (e.g., in smaller data sets with severe multicollinearity). If greater interpretation is necessary and many of the features are redundant or irrelevant then a lasso or elastic net penalty may be preferable.

-   The following uses model.matrix to dummy encode our feature set (Matrix::sparse.model.matrix is used to increase eÔ¨Éciency on larger data sets). We transform the response variable (not a must); however, parametric models such as regularized regression are sensitive to skewed response values so transforming can often improve predictive performance.

```{r}
boston_train_x <- model.matrix(cmedv ~ ., pdp::boston)[, -1]
boston_train_y <- pdp::boston$cmedv

head(boston_train_x)
```

```{r}
boston_ridge_model <- glmnet::glmnet(
  x = boston_train_x,
  y = boston_train_y,
  alpha = 0
)

summary(boston_ridge_model)
```

```{r}
boston_ridge_model$lambda %>% head()
```

```{r}
boston_ridge_model$a0 %>% names()
```

```{r}
lamb_da <- boston_ridge_model$lambda %>%
  as.data.frame() %>%
  mutate(penalty = boston_ridge_model$a0 %>% names()) %>%
  rename(lambda = ".")

head(lamb_da)
```

```{r}
boston_ridge_model$beta %>% head()
```

```{r}
(results <- boston_ridge_model$beta %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(penalty, coefficients, -rowname) %>%
  left_join(lamb_da))
```

```{r}
(result_labels <- results %>%
  group_by(rowname) %>%
  filter(lambda == min(lambda)) %>%
  ungroup() %>%
  top_n(5, wt = abs(coefficients)) %>%
  mutate(var = paste0("x", 1:5)))
```

```{r}
ggplot() +
  geom_line(data = results, aes(lambda, coefficients, group = rowname, colour = rowname), show.legend = FALSE) +
  scale_x_log10() +
  geom_text(data = result_labels, aes(lambda, coefficients, label = var, color = rowname, show.legend = FALSE),  nudge_x = -.06)
```

### Lasso Penalty Regression

```{r}
boston_lasso_model <- glmnet::glmnet(
  x = boston_train_x,
  y = boston_train_y,
  alpha = 1
)
```

```{r}
lamb_da <- boston_lasso_model$lambda %>%
  as.data.frame() %>%
  mutate(penalty = boston_lasso_model$a0 %>% names()) %>%
  rename(lambda = ".")
```

```{r}
results <- boston_lasso_model$beta %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(penalty, coefficients, -rowname) %>%
  left_join(lamb_da)
```

```{r}
(result_labels <- results %>% 
  group_by(rowname) %>%
  filter(lambda == min(lambda)) %>%
  ungroup() %>%
  top_n(5, wt = abs(coefficients)) %>%
  mutate(var = paste0("x",1:5))) %>% head()
```

```{r}
ggplot() +
  geom_line(data = results, aes(lambda, coefficients, colour = rowname, group = rowname), show.legend = FALSE) +
  scale_x_log10() +
  geom_text(data = result_labels, aes(lambda, coefficients, label = var, color = rowname), nudge_x = -.05, show.legend = TRUE)
```

### Elastic Nets

```{r}
boston_elastic_model <- glmnet::glmnet(
  x = boston_train_x,
  y = boston_train_y,
  alpha = .2
)
```

```{r}
lam <- boston_elastic_model$lambda %>% 
  as.data.frame() %>%
  mutate(penalty = boston_elastic_model$a0 %>% names()) %>%
  rename(lambda = ".")
```

```{r}
results <- boston_elastic_model$beta %>% 
  as.matrix() %>% 
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(penalty, coefficients, -rowname) %>%
  left_join(lam)
```

```{r}
result_labels <- results %>%
  group_by(rowname) %>%
  filter(lambda == min(lambda)) %>%
  ungroup() %>%
  top_n(5, wt = abs(coefficients)) %>%
  mutate(var = paste0("x", 1:5))
```

```{r}
ggplot() +
  geom_line(data = results, aes(lambda, coefficients, group = rowname, color = rowname), show.legend = FALSE) +
  scale_x_log10() +
  geom_text(data = result_labels, aes(lambda, coefficients, label = var, color = rowname), nudge_x = -.05, show.legend = FALSE)
```

## Implementation in AmesHousing dataset

```{r}
X <- model.matrix(Sale_Price ~ ., ames_train)[, -1]

Y <- log(ames_train$Sale_Price)
```

-   To apply a regularized model we can use the glmnet::glmnet() function. The alpha parameter tells glmnet to perform a ridge (alpha = 0), lasso (alpha = 1), or elastic net (0 \< alpha \< 1) model. By default, glmnet will do two things that you should be aware of:

    1.  Since regularized methods apply a penalty to the coeÔ¨Écients, we need to ensure our coeÔ¨Écients are on a common scale. If not, then predictors with naturally larger values (e.g., total square footage) will be penalized more than predictors with naturally smaller values (e.g., total number of rooms). By default, glmnet automatically standardizes your features. If you standardize your predictors prior to glmnet you can turn this argument oÔ¨Ä with standardize = FALSE.

    2.  glmnet will Ô¨Åt ridge models across a wide range of ùúÜ values.

```{r}
ridge_model <- glmnet::glmnet(
  x = X,
  y = Y,
  alpha = 0,
  standardize = TRUE
)

plot(ridge_model, xvar = "lambda")
```

-   You can see how the largest ùúÜ value has pushed most of these coeÔ¨Écients to nearly 0.

```{r}
coef(ridge_model)[c("Latitude", "Overall_QualVery_Excellent"), 100]
```

```{r}
coef(ridge_model)[c("Latitude", "Overall_QualVery_Excellent"), 1]
```

```{r}
# Apply CV ridge regression to Ames data
ridge <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# Apply CV lasso regression to Ames data
lasso <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1
)

# plot results
par(mfrow = c(1, 2))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
```

```{r}
# Ridge model
ridge_min <- glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# Lasso model
lasso_min <- glmnet(
  x = X,
  y = Y,
  alpha = 1
)

par(mfrow = c(1, 2))

# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "red", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "blue", lty = "dashed")

# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Lasso penalty\n\n")
abline(v = log(lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "blue", lty = "dashed")
```

```{r}
# for reproducibility
set.seed(40)
# grid search across
cv_glmnet <- train(
  x = X,
  y = Y,
  method = "glmnet",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)
```

```{r}
cv_glmnet$bestTune
```

```{r}
ggplot(cv_glmnet)
```

```{r}
# predict Sale Price on training data
predictions <- predict(cv_glmnet, X)

# compute RMSE of transormed predicted
RMSE(exp(predictions), exp(Y))
```

```{r}
pfun <- function(object, newdata) predict(object, newdata = newdata)

vip(cv_glmnet, num_features = 20, geom = "point", aesthetics = list(color = "red", size = 2))
```

```{r}
pdp::partial(cv_glmnet, "Garage_Cars", grid.resolution = 40, plot = TRUE)
```

```{r}
pdp::partial(cv_glmnet, pred.var = "Gr_Liv_Area", grid.resolution = 20)
```

```{r}
plot_1 <- pdp::partial(cv_glmnet, pred.var = "Gr_Liv_Area", grid.resolution = 20) %>%
  mutate(yhat = exp(yhat)) %>%
  ggplot(aes(Gr_Liv_Area, yhat)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)
```

```{r}
plot_2 <- pdp::partial(cv_glmnet, pred.var = "Overall_QualExcellent", grid.resolution = 20) %>%
  mutate(yhat = exp(yhat),
         Overall_QualExcellent = factor(Overall_QualExcellent)
  ) %>%
  ggplot(aes(Overall_QualExcellent, yhat)) +
  geom_boxplot() +
  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)
```

```{r}
plot_3 <- pdp::partial(cv_glmnet, pred.var = "First_Flr_SF", grid.resolution = 20) %>%
  mutate(yhat = exp(yhat)) %>%
  ggplot(aes(First_Flr_SF, yhat)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)
```

```{r}
plot_4 <- pdp::partial(cv_glmnet, pred.var = "Garage_Cars") %>%
  mutate(yhat = exp(yhat)) %>%
  ggplot(aes(Garage_Cars, yhat)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)
```

```{r}
gridExtra::grid.arrange(plot_1, plot_2, plot_3, plot_4, nrow = 2)
```

```{r}
pdp::partial(cv_glmnet, pred.var = "Overall_QualPoor") %>%
  mutate(
    yhat = exp(yhat),
    Overall_QualPoor = factor(Overall_QualPoor)
    ) %>%
  ggplot(aes(Overall_QualPoor, yhat)) +
  geom_boxplot() +
  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)
```

```{r}
library(readr)
attrition <- read_csv("/home/jeremy/Work/Data Science/Code in Zip/homlr-master/data/attrition.csv")

df <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)

# Create training (70%) and test (30%) sets for the
# rsample::attrition data. Use set.seed for reproducibility
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
train <- training(churn_split)
test <- testing(churn_split)

# train logistic regression model
glm_mod <- train(
  Attrition ~ .,
  data = train,
  method = "glm",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10)
)
  
# train regularized logistic regression model
penalized_mod <- train(
  Attrition ~ .,
  data = train,
  method = "glmnet",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)
```

```{r}
summary(resamples(
  list(
    logistic_model = glm_mod,
    penalized_model = penalized_mod
  )
))$statistics$Accuracy
```
