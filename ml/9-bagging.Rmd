---
title: "9-random-forest"
output: html_document
---

```{r}
library(tidyverse)
library(plotly)
library(doParallel)
library(foreach)
library(caret)
library(rpart)
library(vip)
library(pdp)
```

-   Optimal performance is often found by bagging 50‚Äì500 trees. Data sets that have a few strong predictors typically require less trees; whereas data sets with lots of noise or multiple strong predictors may need more. Using too many trees will not lead to overÔ¨Åtting. However, it‚Äôs important to realize that since multiple models are being run, the more iterations you perform the more computational and time requirements you will have. As these demands increase, performing k-fold CV can become computationally burdensome.

-   A beneÔ¨Åt to creating ensembles via bagging, which is based on resampling with replacement, is that it can provide its own internal estimate of predictive performance with the out-of-bag (OOB) sample. The OOB sample can be used to test predictive performance and the results usually compare well compared to k-fold CV assuming your data set is suÔ¨Éciently large (say ùëõ ‚â• 1, 000). Consequently, as your data sets become larger and your bagging iterations increase, it is common to use the OOB error estimate as a proxy for predictive performance.

```{r}
ames <- AmesHousing::make_ames()

ames$Date <- as.Date(paste(ames$Year_Sold, ames$Mo_Sold, "01", sep = "-"))

set.seed(40)  # for reproducibility
split  <- rsample::initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- rsample::training(split)
ames_test   <- rsample::testing(split)
```

```{r}
set.seed(40)

ames_bag1 <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "treebag",
  nbagg = 100,
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)

ames_bag1
```

```{r}
# using ranger to do the same as above.  Will allow for bagging under 10 trees
# and is much faster!
ntree <- seq(1, 200, by = 2)
# create empty vector to store OOB RMSE values
rmse <- vector(mode = "numeric", length = length(ntree))

for (i in seq_along(ntree)) {
  # reproducibility
  set.seed(123)
  # perform bagged model
  model <- ranger::ranger(
  formula = Sale_Price ~ .,
  data    = ames_train,
  num.trees = ntree[i],
  mtry = ncol(ames_train) - 1,
  min.node.size = 1
)
  # get OOB error
  rmse[i] <- sqrt(model$prediction.error)
}

bagging_errors <- data.frame(ntree, rmse)

ggplot(bagging_errors, aes(ntree, rmse)) +
  geom_line() +
  geom_hline(yintercept = 41019, lty = "dashed", color = "grey50") +
  annotate("text", x = 100, y = 41385, label = "Best individual pruned tree", 
           vjust = 0, hjust = 0, color = "grey50") +
  annotate("text", x = 100, y = 26750, label = "Bagged trees", vjust = 0, hjust = 0) +
  ylab("RMSE") +
  xlab("Number of trees")
```

```{r}
ames_bag2 <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "treebag",
  trControl = trainControl(method = "cv", number = 10),
  nbagg = 200,
  control = rpart.control(minsplit = 2, cp = 0)
)

ames_bag2
```

```{r}
# Create a parallel socket cluster
cl <- makeCluster(8) # use 8 workers
registerDoParallel(cl) # register the parallel backend
```

```{r}
# Fit trees in parallel and compute predictions on the test set
predictions <- foreach(
  icount(160),
  .packages = "rpart",
  .combine = cbind
  ) %dopar% {
    # bootstrap copy of training data
    index <- sample(nrow(ames_train), replace = TRUE)
    ames_train_boot <- ames_train[index, ]
    
    # fit tree to bootstrap copy
    bagged_tree <- rpart(
      Sale_Price ~ .,
      control = rpart.control(minsplit = 2, cp = 0),
      data = ames_train_boot
    )
    
    predict(bagged_tree, newdata = ames_test)
}
predictions[1:5, 1:7]
```

```{r}
predictions %>%
  as.data.frame() %>%
  mutate(
    observation = 1:n(),
    actual = ames_test$Sale_Price
  ) %>%
  tidyr::gather(tree, predicted, -c(observation, actual)) %>%
  group_by(observation) %>%
  mutate(tree = stringr::str_extract(tree, '\\d+') %>% as.numeric()) %>%
  ungroup() %>%
  arrange(observation, tree) %>%
  group_by(observation) %>%
  mutate(avg_prediction = cummean(predicted)) %>%
  group_by(tree) %>%
  summarize(RMSE = RMSE(avg_prediction, actual)) %>%
  ggplot(aes(tree, RMSE)) +
  geom_line() +
  xlab('Number of trees')
```

```{r}
# Shutdown parallel cluster
stopCluster(cl)
```

```{r}
vip(ames_bag2, num_features = 40, bar = FALSE)
```

```{r}
# Construct partial dependence plots
p1 <- pdp::partial(
  ames_bag2,
  pred.var = 'Lot_Area',
  grid.resolution = 20
) %>% autoplot()

p2 <- pdp::partial(
  ames_bag2,
  pred.var = "Lot_Frontage",
  grid.resolution = 20
) %>% autoplot()
```

```{r}
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

```{r}

```
