---
title: "3-linear-regression"
output: html_document
---

```{r}
library(tidyverse)
library(caret)
library(TSstudio)
library(h2o)
library(naniar)
library(vip)
library(recipes)
library(visdat)
```

```{r}
h2o.no_progress()
h2o.init(max_mem_size = "4G")
```

```{r}
ames <- AmesHousing::make_ames()

set.seed(40)  # for reproducibility
split  <- rsample::initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- rsample::training(split)
ames_test   <- rsample::testing(split)
```

```{r}
lm1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)
summary(lm1)
```

-   Most statistical software, including R, will include estimated standard errors, t-statistics, etc. as part of its regression output. However, it is important to remember that such quantities depend on three major assumptions of the linear regresion model:

    1.  Independent observations

    2.  The random errors have mean zero, and constant variance

    3.  The random errors are normally distributed

```{r}
sigma(lm1)  # RMSE
sigma(lm1) ^ 2  # MSE
```

```{r}
confint(lm1, level = .95)
```

```{r}
(lm2 <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train))
```

```{r}
(l2 <- update(lm1, . ~ . + Year_Built))
```

```{r}
summary(l2)
```

```{r}
lm3 <- lm(Sale_Price ~ ., data = ames_train)

broom::tidy(lm3)
```

```{r}
(cv_model1 <- train(
  form = Sale_Price ~ Gr_Liv_Area,
  data = ames_train,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))
```

-   The resulting cross-validated RMSE is \$56,220.6 (this is the average RMSE across the 10 CV folds). How should we interpret this? When applied to unseen data, the predictions this model makes are, on average, about \$56,220.6 oï¬€ from the actual sale price.

-   We can perform cross-validation on the other two models in a similar fashion, which we do in the code chunk below.

```{r}
(cv_model2 <- train(
  Sale_Price ~ Gr_Liv_Area + Year_Built,
  data = ames_train,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))
```

```{r}
cv_model3 <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
)
```

```{r}
# Extract out of sample performance measures
summary(resamples(list(
  model1 = cv_model1,
  model2 = cv_model2,
  model3 = cv_model3
)))
```

-   Extracting the results for each model, we see that by adding more information via more predictors, we are able to improve the out-of-sample cross validation performance metrics. Speciï¬cally, our cross-validated RMSE reduces from \$46,132.74 (the model with two predictors) down to \$37,304.33 (for our full model). In this case, the model with all possible main eï¬€ects performs the â€œbestâ€ (compared with the other two).

## Model Concerns

**Linear relationship**

-   Linear regression assumes a linear relationship be- tween the predictor and the response variable. However, as discussed in Chapter 3, non-linear relationships can be made linear (or near-linear) by applying transformations to the response and/or predictors.

```{r}
p1 <- ggplot(ames_train, aes(Year_Built, Sale_Price)) +
  geom_point(size = 1, alpha = .4) +
  geom_smooth(se = FALSE) +
  scale_y_continuous("Sale price", labels = scales::dollar) +
  xlab("Year built") +
  ggtitle(paste("Non-transformed variables with a\n", "non-linear relationship."))

p2 <- ggplot(ames_train, aes(Year_Built, Sale_Price)) +
  geom_point(size = 1, alpha = .4) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_log10("Sale price", labels = scales::dollar,
  breaks = seq(0, 400000, by = 100000)) +
  xlab("Year built") +
  ggtitle(paste("Transforming variables can provide a\n", "near-linear relationship."))

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

**Constant variance among residuals (Homoscedasticity)**

-   Linear regression assumes the variance among error terms (ğœ–1 , ğœ–2 , â€¦ , ğœ–uï¿½ ) are constant (this assumption is referred to as homoscedasticity). If the error variance is not constant, the p-values and conï¬dence intervals for the coeï¬ƒcients will be invalid. Similar to the linear relationship assumption, non-constant variance can often be resolved with variable transformations or by including additional predictors.

```{r}
df1 <- broom::augment(cv_model1$finalModel, data = ames_train)

p1 <- ggplot(df1, aes(.fitted, .std.resid)) +
  geom_point(size = 1, alpha = .4) +
  xlab("Predicted values") +
  ylab("Residuals") +
  ggtitle("Model 1", subtitle = "Sale_Price ~ Gr_Liv_Area")

df2 <- broom::augment(cv_model3$finalModel, data = ames_train)

p2 <- ggplot(df2, aes(.fitted, .std.resid)) +
  geom_point(size = 1, alpha = .4) +
  xlab("Predicted values") +
  ylab("Residuals") +
  ggtitle("Model 3", subtitle = "Sale_Price ~ .")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

#### No Autocorrelation

-   Linear regression assumes the errors are independent and uncorrelated. If in fact, there is correlation among the errors, then the estimated standard errors of the coeï¬ƒcients will be biased leading to prediction intervals being narrower than they should be.

```{r}
df1 <- mutate(df1, id = row_number())
df2 <- mutate(df2, id = row_number())

p1 <- ggplot(df1, aes(id, .std.resid)) +
  geom_point(size = 1, alpha = .4) +
  xlab("Row ID") +
  ylab("Residuals") +
  ggtitle("Model 1", subtitle = "Correlated residuals.")

p2 <- ggplot(df2, aes(id, .std.resid)) +
  geom_point(size = 1, alpha = .4) +
  xlab("Row ID") +
  ylab("Residuals") +
  ggtitle("Model 3", subtitle = "Uncorrelated residuals.")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

#### No or little Multicollinearity

-   Collinearity refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in the OLS, since it can be diï¬ƒcult to separate out the individual eï¬€ects of collinear variables on the response. In fact, collinearity can cause predictor variables to appear as statistically insigniï¬cant when in fact they are signiï¬cant. This obviously leads to an inaccurate interpretation of coeï¬ƒcients and makes it diï¬ƒcult to identify inï¬‚uential predictors.

-   In ames, for example, Garage_Area and Garage_Cars are two variables that have a correlation of 0.89 and both variables are strongly related to our response variable (Sale_Price). Looking at our full model where both of these variables are included, we see that Garage_Cars is found to be statistically signiï¬cant but Garage_Area is not:

```{r}
cor(ames[-1, ])

```

```{r}
# fit with two strongly correlated variables
summary(cv_model3) %>%
  broom::tidy() %>%
  filter(term %in% c("Garage_Area", "Garage_Cars"))
```

-   However, if we reï¬t the full model without Garage_Cars, the coeï¬ƒcient estimate for Garage_Area increases two fold and becomes statistically signiï¬cant.

```{r}
mod_wo_Garage_Cars <- train(
  Sale_Price ~ .,
  data = select(ames_train, -Garage_Cars),
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
)

summary(mod_wo_Garage_Cars) %>%
  broom::tidy() %>%
  filter(term == "Garage_Area")
```

## Principal Component Regression

```{r}
cv_model_pcr <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "pcr",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
)
```

```{r}
cv_model_pcr$bestTune
```

```{r}
ggplot(cv_model_pcr)
```

## Partial Least Squares

```{r}
cv_model_pls <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "pls",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
)
```

```{r}
cv_model_pls$bestTune
```

```{r}
ggplot(cv_model_pls)
```

## Feature Interpretation

```{r}
vip(cv_model_pls, num_features = 20, method = "model")
```

```{r}
pdp::partial(cv_model_pls, "Gr_Liv_Area", grid.resolution = 20, plot = TRUE)
```

```{r}
pdp::partial(cv_model_pls, "Garage_Cars", grid.resolution = 20, plot = TRUE)
```
