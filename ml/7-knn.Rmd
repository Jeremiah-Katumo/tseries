---
title: "6-knn"
output: html_document
---

```{r}
library(tidyverse)
library(rsample)
library(recipes)
library(h2o)
library(caret)
library(naniar)
library(readr)
library(ggmap)
```

```{r}
h2o.no_progress()
h2o.init(max_mem_size = "4G")
```

```{r}
ames <- AmesHousing::make_ames()

ames$Date <- as.Date(paste(ames$Year_Sold, ames$Mo_Sold, "01", sep = "-"))

set.seed(40)  # for reproducibility
split  <- rsample::initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- rsample::training(split)
ames_test   <- rsample::testing(split)
```

```{r}
set.seed(40)

attrition <- read_csv("/home/jeremy/Work/Data Science/Code in Zip/homlr-master/data/attrition.csv")
head(attrition)
```

```{r}
(attrition_df <- attrition %>%
  mutate_if(is.ordered, factor, ordered = FALSE))
```

```{r}
churn_split <- initial_split(attrition_df, prop = .7, strata = "Attrition")

churn_train <- training(churn_split)
churn_test <- testing(churn_split)
```

```{r}
mnist <- dslabs::read_mnist()
names(mnist)
```

```{r}
ames_train_prep <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
  prep(training = ames_train, retain = TRUE) %>%
  juice() %>%
  select(-Sale_Price)

ames_train_prep
```

```{r}
ames_train_prep[-home, ]
```

```{r}
ames_train_prep[home, ]
```

```{r}
home <- 30
k = 10
index <- as.vector(FNN::knnx.index(ames_train_prep[-home, ], ames_train_prep[home, ], k = k))
knn_homes <- ames_train[c(home, index), ]
```

```{r}
knn_homes %>% 
  select(Longitude, Latitude) %>%
  mutate(desc = factor(c('House of interest', rep('Closest neighbors', k)), 
                       levels = c('House of interest', 'Closest neighbors'))) %>%
  qmplot(Longitude, Latitude, data = ., 
         maptype = "toner-background", darken = .7, color = desc, size = I(2.5)) + 
  theme(legend.position = "top",
        legend.title = element_blank())
```

## Measuring Similarity

-   The KNN algorithm identiÔ¨Åes ùëò observations that are ‚Äúsimilar‚Äù or nearest to the new record being predicted and then uses the average response value (regression) or the most common class (classiÔ¨Åcation) of those ùëò observations as the predicted output.

### Distance Measures

-   The most common distance measures are Euclidean and Manhattan distance metrics.

-   Euclidean distance measures the straight line distance between two samples.

-   Manhattan distance measures the point-to-point travel time and is commonly used for binary predictors (e.g one hot-encoded 0/1 indicator variables).

```{r}
two_houses <- ames_train %>%
  select(Gr_Liv_Area, Year_Built) %>%
  sample_n(2)

two_houses
```

```{r}
dist(two_houses, method = "euclidean")
```

```{r}
dist(two_houses, method = "manhattan")
```

```{r}
p1 <- ggplot(two_houses, aes(Gr_Liv_Area, Year_Built)) +
  geom_point() +
  geom_line(lty = "dashed") +
  ggtitle("Euclidean Distance")

p2 <- ggplot(two_houses, aes(Gr_Liv_Area, Year_Built)) +
  geom_point() +
  geom_step(lty = "dashed") +
  ggtitle("Manhattan Distance")

gridExtra::grid.arrange(p1, p1, nrow = 1)
```

### Pre-processing

```{r}
( 
  home1 <- ames %>%
    mutate(id = row_number()) %>%
    select(Bedroom_AbvGr, Year_Built, id) %>%
    filter(Bedroom_AbvGr == 4 & Year_Built == 2008) %>%
    slice(1) %>%
    mutate(home = "home1") %>%
    select(home, everything())
)
```

```{r}
( 
  home2 <- ames %>%
    mutate(id = row_number()) %>%
    select(Bedroom_AbvGr, Year_Built, id) %>%
    filter(Bedroom_AbvGr == 2 & Year_Built == 2008) %>%
    slice(1) %>%
    mutate(home = "home2") %>%
    select(home, everything())
)
 
( 
  home3 <- ames %>%
    mutate(id = row_number()) %>%
    select(Bedroom_AbvGr, Year_Built, id) %>%
    filter(Bedroom_AbvGr == 3 & Year_Built == 1998) %>%
    slice(1) %>%
    mutate(home = "home3") %>%
    select(home, everything())
)
```

```{r}
features <- c("Bedroom_AbvGr", "Year_Built")

# distance between home 1 and 2
(dist(rbind(home1[,features], home2[,features])))

(dist(rbind(home1[,features], home3[,features])))
```

-   The Euclidean distance between home1 and home3 is larger due to the larger diÔ¨Äerence in Year_Built with home2.

```{r}
(
  scaled_ames <- recipe(Sale_Price ~ ., ames_train) %>%
    step_center(all_numeric()) %>%
    step_scale(all_numeric()) %>%
    prep(training = ames, retain = TRUE) %>% 
    juice()
)
```

```{r}
(  
  home1_std <- scaled_ames %>%
    mutate(id = row_number()) %>%
    filter(id == home1$id) %>%
    select(Bedroom_AbvGr, Year_Built, id) %>%
    mutate(home = "home1") %>%
    select(home, everything())
)

(
  home2_std <- scaled_ames %>%
    mutate(id = row_number()) %>%
    filter(id == home2$id) %>%
    select(Bedroom_AbvGr, Year_Built, id) %>%
    mutate(home = "home2") %>%
    select(home, everything())
)

(
  home3_std <- scaled_ames %>%
    mutate(id = row_number()) %>%
    filter(id == home3$id) %>%
    select(Bedroom_AbvGr, Year_Built, id) %>%
    mutate(home = "home3") %>%
    select(home, everything())
)
```

```{r}
dist(rbind(home1_std[,features], home2_std[,features]))
```

```{r}
dist(rbind(home1_std[,features], home3_std[,features]))
```

## Choosing k

```{r}
churn_train %>% names()
```

```{r}
# Create a blueprint 
(
  blueprint <- recipe(Attrition ~ ., churn_train) %>%
    step_nzv(all_nominal()) %>%
    step_integer(contains("Satisfaction")) %>%
    step_integer(WorkLifeBalance) %>%
    step_integer(JobInvolvement) %>%
    step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
    step_center(all_numeric(), -all_outcomes()) %>%
    step_scale(all_numeric(), -all_outcomes()) 
)
```

```{r}
# Create a resampling method
cv <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)
```

```{r}
# Create a hyperparameter grid search
hyper_grid <- expand.grid(
  k = floor(seq(1, nrow(churn_train)/3, length.out = 20))
)
```

```{r}
# Fit knn model and perform grid search
knn_grid <- train(
  blueprint,
  data = churn_train,
  method = "knn",
  trControl = cv,
  tuneGrid = hyper_grid,
  metric = "ROC"
)

knn_grid
```

```{r}
ggplot(knn_grid)
```

## MNIST

```{r}
index <- sample(nrow(mnist$train$images), size = 10000)
mnist_x <- mnist$train$images[index, ]
mnist_y <- factor(mnist$train$labels[index])
```

```{r}
mnist_x %>%
  as.data.frame() %>%
  map_df(sd) %>%
  gather(feature, sd) %>%
  ggplot(aes(sd)) +
  geom_histogram(binwidth = 1)
```

```{r}
# Rename features
colnames(mnist_x) <- paste0("V", 1:ncol(mnist_x))

# Remove near zero variance features manually
nzv <- nearZeroVar(mnist_x)
index <- setdiff(1:ncol(mnist_x), nzv)
mnist_x <- mnist_x[, index]
```

```{r}
cv <- trainControl(
  method = "LGOCV",
  p = .7,
  number = 1,
  savePredictions = TRUE
)

hyper_grid <- expand.grid(k = seq(3, 25, by = 2))

mnist_knn <- train(
  mnist_x, mnist_y,
  method = "knn",
  tuneGrid = hyper_grid,
  preProc = c("center", "scale"),
  trControl = cv
)

ggplot(mnist_knn)
```

```{r}
# Create confusion matrix
cm <- confusionMatrix(mnist_knn$pred$pred, mnist_knn$pred$obs)
cm
```

```{r}
cm$byClass[, c(1:2, 11)] # sensitivity, specificity, & accuracy
```

```{r}
# Top 20 most important features
vi <- varImp(mnist_knn)
vi
```

```{r}
# Get median value for feature importance
imp <- vi$importance %>%
  rownames_to_column(var = "feature") %>%
  gather(response, imp, -feature) %>%
  group_by(feature) %>%
  summarize(imp = median(imp))

imp
```

```{r}
# Create tibble for all edge pixels
edges <- tibble(
  feature = paste0("V", nzv),
  imp = 0
)
edges
```

```{r}
# Combine and plot
imp <- rbind(imp, edges) %>%
  mutate(ID = as.numeric(str_extract(feature, "\\d+"))) %>%
  arrange(ID)

image(matrix(imp$imp, 28, 28), col = gray(seq(0, 1, 0.05)), xaxt="n", yaxt="n")
```

```{r}
# Get a few accurate predictions
good <- mnist_knn$pred %>%
  filter(pred == obs) %>%
  sample_n(4)

# Get a few inaccurate predictions
bad <- mnist_knn$pred %>%
  filter(pred != obs) %>%
  sample_n(4)

combine <- bind_rows(good, bad)
```

```{r}
# Get original feature set with all pixel features
index <- sample(nrow(mnist$train$images), 10000)
X <- mnist$train$images[index,]

# Plot results
par(mfrow = c(4, 2), mar=c(1, 1, 1, 1))
layout(matrix(seq_len(nrow(combine)), 4, 2, byrow = FALSE))
for(i in seq_len(nrow(combine))) {
  image(matrix(X[combine$rowIndex[i],], 28, 28)[, 28:1],
    col = gray(seq(0, 1, 0.05)),
    main = paste("Actual:", combine$obs[i], " ", "Predicted:", combine$pred[i]), xaxt="n", yaxt="n")
}
```
